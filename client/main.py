"""mcp-codemode client — FastAPI server with /chat endpoint.

Uses PydanticAI with MCPServerStreamableHTTP to drive an agentic
tool-use loop against the mcp-codemode MCP server.
"""

from __future__ import annotations

import logging
from contextlib import asynccontextmanager
from typing import Any

import logfire
from fastapi import FastAPI
from pydantic import BaseModel, Field
from pydantic import TypeAdapter
from pydantic_ai.messages import ModelMessage

from agent import AgentResponse, mcp_server, run_agent
from config import settings
from proxy import github_router, graph_router

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
)
logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Keep the MCP server connection open for the app's lifetime."""
    async with mcp_server:
        yield


app = FastAPI(
    title="mcp-codemode client",
    description="LLM-powered chat that uses MCP tools for sandboxed code execution",
    lifespan=lifespan,
)

# ── Proxy routers (used by sandbox to reach third-party APIs) ────────
app.include_router(graph_router, prefix="/graph")
app.include_router(github_router, prefix="/github")


# ── Request / Response models ────────────────────────────────────────

class ChatRequest(BaseModel):
    message: str = Field(..., description="The user message to send to the agent")
    conversation_history: list[dict[str, Any]] | None = Field(
        default=None,
        description=(
            "Optional prior messages (PydanticAI ModelMessage format) "
            "for multi-turn context."
        ),
    )
    use_code_exec_agent: bool = Field(
        default=False,
        description=(
            "Whether to use the code-execution agent with broader tool access "
            "or the no-code agent with only non-code tools."
        ),
    )


class ToolCallInfo(BaseModel):
    round: int
    tool: str
    input: dict[str, Any]
    output: str
    is_error: bool = False


class ChatResponse(BaseModel):
    response: str = Field(..., description="The agent's final text response")
    input_tokens: int = Field(..., description="Total input tokens consumed by the agent")
    total_tokens: int = Field(..., description="Total tokens consumed by the agent (input + output)")
    output_tokens: int = Field(..., description="Total output tokens generated by the agent")
    tool_calls: list[ToolCallInfo] = Field(
        default_factory=list,
        description="Log of tool calls made during this request",
    )
    rounds: int = Field(0, description="Number of LLM requests made")


# ── Endpoints ────────────────────────────────────────────────────────

@app.post("/chat", response_model=ChatResponse)
async def chat(req: ChatRequest) -> ChatResponse:
    """Send a message to the LLM agent which can execute code via MCP tools.

    PydanticAI handles the full tool-use loop: the LLM calls MCP tools,
    gets results, and iterates until it produces a final text answer
    (or hits the safety limit on rounds).
    """
    logger.info("Received chat request: %.120s", req.message)

    # Deserialize conversation history if provided
    message_history = None
    if req.conversation_history:

        ta = TypeAdapter(list[ModelMessage])
        message_history = ta.validate_python(req.conversation_history)

    result: AgentResponse = await run_agent(
        req.message,
        message_history=message_history,
        use_code_exec_agent=req.use_code_exec_agent,
    )

    return ChatResponse(
        response=result.text,
        input_tokens=result.input_tokens,
        output_tokens=result.output_tokens,
        total_tokens=result.total_tokens,
        tool_calls=[ToolCallInfo(**tc) for tc in result.tool_calls],
        rounds=result.rounds,
    )


@app.get("/tools")
async def list_tools():
    """Return the MCP tools currently available to the agent."""
    tools = await mcp_server.list_tools()
    return {
        "tools": [
            {
                "name": t.name,
                "description": t.description or "",
                "parameters": t.inputSchema,
            }
            for t in tools
        ]
    }


@app.get("/health")
async def health():
    """Health check."""
    return {"status": "ok", "mcp_url": settings.mcp_server_url}


if settings.logfire_token:
    logfire.configure(
        token=settings.logfire_token,
        service_name="mcp-codemode-client",
        environment="test",
    )
    logfire.instrument_httpx(capture_request_body=True, capture_response_body=True)
    logfire.instrument_fastapi(app)
